{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e45456a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_import\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting and Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Profiling for EDA (Exploratory Data Analysis)\n",
    "import ydata_profiling  # or use 'import pandas_profiling' if you prefer\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modeling\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingClassifier, StackingRegressor, VotingClassifier, VotingRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# Metrics and Evaluation\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Warnings to avoid unwanted messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Display helper for Jupyter (to display the dataframe)\n",
    "from IPython.display import display\n",
    "\n",
    "class tools:\n",
    "    @staticmethod\n",
    "    def display_dataframe_to_user(name, dataframe):\n",
    "        print(f\"\\n{name}:\")\n",
    "        display(dataframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0463860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_visualize\n",
    "\n",
    "# Data Profiling for Exploratory Data Analysis (EDA)\n",
    "def data_profiling(df):\n",
    "    # Using ydata_profiling to generate a detailed profile report\n",
    "    profile = ydata_profiling.ProfileReport(df, title=\"Data Profiling Report\", explorative=True)\n",
    "    profile.to_file(\"data_profiling_report.html\")\n",
    "    print(\"Data profiling report generated as 'data_profiling_report.html'\")\n",
    "\n",
    "# Visualizing data and detecting columns\n",
    "def data_visualization(df, target_column=None):\n",
    "    \"\"\"\n",
    "    Visualizes data and detects columns.\n",
    "    If target_column is None, it defaults to the last column.\n",
    "    \"\"\"\n",
    "    # 1. Determine the target column immediately\n",
    "    if target_column is None:\n",
    "        target_column = df.columns[-1]\n",
    "        print(f\"Target column not specified. Auto-selecting last column: '{target_column}'\")\n",
    "    else:\n",
    "        print(f\"Target column manually set to: '{target_column}'\")\n",
    "\n",
    "    # 2. Detect Numeric and Categorical Columns\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_columns = df.select_dtypes(include=[object]).columns.tolist()\n",
    "    \n",
    "    # Remove high-cardinality categorical columns (likely IDs)\n",
    "    cat_col_del = []\n",
    "    for cat_col in categorical_columns:\n",
    "        if df[cat_col].nunique() >= 0.5 * len(df[cat_col]):\n",
    "            df.drop(columns=[cat_col], inplace=True)\n",
    "            cat_col_del.append(cat_col)\n",
    "    \n",
    "    categorical_columns = [col for col in categorical_columns if col not in cat_col_del]       \n",
    "    \n",
    "    # 3. Generate Profile\n",
    "    data_profiling(df)\n",
    "    \n",
    "    # 4. Plots\n",
    "    # Box plot for outliers\n",
    "    if numeric_columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        n_cols = len(numeric_columns)\n",
    "        n_rows = (n_cols // 3) + (1 if n_cols % 3 > 0 else 0)\n",
    "        for i, col in enumerate(numeric_columns):\n",
    "            plt.subplot(n_rows, 3, i+1)\n",
    "            sns.boxplot(x=df[col])\n",
    "            plt.title(f\"Boxplot of {col}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Histograms for distribution\n",
    "    if numeric_columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for i, col in enumerate(numeric_columns):\n",
    "            plt.subplot(n_rows, 3, i+1)\n",
    "            sns.histplot(df[col], kde=True)\n",
    "            plt.title(f\"Histogram of {col}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Bar plot for categorical columns\n",
    "    if categorical_columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        n_cols_cat = len(categorical_columns)\n",
    "        n_rows_cat = (n_cols_cat // 3) + (1 if n_cols_cat % 3 > 0 else 0)\n",
    "        for i, col in enumerate(categorical_columns):\n",
    "            plt.subplot(n_rows_cat, 3, i+1)\n",
    "            sns.countplot(x=df[col])\n",
    "            plt.title(f\"Bar plot of {col}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Correlation Matrix\n",
    "    if numeric_columns:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        corr_matrix = df[numeric_columns].corr()\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "        plt.title('Correlation Matrix')\n",
    "        plt.show()\n",
    "\n",
    "    return target_column, categorical_columns, numeric_columns\n",
    "\n",
    "# [Keep the other plotting functions: plot_regression_results, plot_confusion_matrix, etc. as they were]\n",
    "# ... (Paste the rest of your original plotting functions here if needed) ...\n",
    "def plot_regression_results(y_true, y_pred):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(y_true, y_pred, color='blue', alpha=0.6)\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], color='red', linestyle='--')\n",
    "    plt.title('Regression: Actual vs Predicted')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_feature_importance(model, feature_names):\n",
    "    importance = model.feature_importances_\n",
    "    feature_importance = pd.Series(importance, index=feature_names).sort_values(ascending=False)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    feature_importance.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "def plot_class_distribution(y):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.countplot(x=y, palette='Set2')\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "# --- NEW: Time Series Evaluation ---\n",
    "def evaluate_time_series_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Plots the forecast against the actual values in a timeline format.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"R^2 Score: {r2:.4f}\")\n",
    "\n",
    "    # Plotting Timeline\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(len(y_test)), y_test.values, label='Actual', color='blue')\n",
    "    plt.plot(range(len(y_pred)), y_pred, label='Predicted', color='orange', linestyle='--')\n",
    "    plt.title('Time Series Forecasting: Actual vs Predicted')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d30e7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_preprocess\n",
    "\n",
    "# Function to preprocess data: Handling missing values, scaling, encoding\n",
    "def preprocess_data(df, target_column, categorical_columns, numeric_columns, time_series=False, date_column=None):\n",
    "    \"\"\"\n",
    "    Preprocesses the input data. If time_series=True, it splits data chronologically.\n",
    "    \"\"\"\n",
    "    # If it's a time series, handle the date column\n",
    "    if time_series and date_column:\n",
    "        print(f\"Sorting data by time column: {date_column}\")\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "        df = df.sort_values(by=date_column)\n",
    "        # We usually drop the date column from features for standard ML models \n",
    "        # as they can't process timestamp objects directly, \n",
    "        # but the order is preserved in the index/rows.\n",
    "        df = df.drop(columns=[date_column])\n",
    "        \n",
    "        # Remove date_column from lists if present\n",
    "        if date_column in categorical_columns: categorical_columns.remove(date_column)\n",
    "        if date_column in numeric_columns: numeric_columns.remove(date_column)\n",
    "\n",
    "    # Exclude the target column\n",
    "    categorical_columns = [col for col in categorical_columns if col != target_column]\n",
    "    numeric_columns = [col for col in numeric_columns if col != target_column]\n",
    "    \n",
    "    # Split data into features (X) and target (y)\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    numeric_features = numeric_columns\n",
    "    categorical_features = categorical_columns\n",
    "    \n",
    "    # Preprocessing pipelines\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    if not categorical_columns:\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[('num', numeric_transformer, numeric_features)]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    # Apply transformations\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # --- TIME SERIES SPLIT LOGIC ---\n",
    "    if time_series:\n",
    "        # split WITHOUT shuffling to preserve order\n",
    "        print(\"Splitting data chronologically (Time Series mode)...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, shuffle=False)\n",
    "    else:\n",
    "        # Standard random split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78948f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_pipeline\n",
    "\n",
    "# Function to train and evaluate multiple models with hyperparameter tuning\n",
    "def train_models(X_train, X_test, y_train, y_test, task_type='classification', search_type='grid', time_series=False):\n",
    "    \"\"\"\n",
    "    Trains and evaluates multiple models based on task type (classification or regression) with hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    \n",
    "    models = {}\n",
    "    best_models = {}\n",
    "    best_params = {}\n",
    "    best_scores = {}\n",
    "\n",
    "    # REMOVED: Internal Label Encoding. \n",
    "    # We now assume y_train and y_test are already encoded before being passed here.\n",
    "\n",
    "    if task_type == 'classification':\n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(),\n",
    "            'SVM': SVC(),\n",
    "            'Decision Tree': DecisionTreeClassifier(),\n",
    "            'Random Forest': RandomForestClassifier(),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(),\n",
    "            'XGBoost': XGBClassifier(),\n",
    "            'Voting Classifier': VotingClassifier(estimators=[\n",
    "                ('lr', LogisticRegression()),\n",
    "                ('rf', RandomForestClassifier()),\n",
    "                ('svm', SVC())\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        param_grids = {\n",
    "            'Logistic Regression': {\n",
    "                'C': [0.01, 0.1, 1, 10],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'solver': ['liblinear']\n",
    "            },\n",
    "            'SVM': {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'gamma': ['scale', 'auto']\n",
    "            },\n",
    "            'Decision Tree': {\n",
    "                'max_depth': [10, 20, 30, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            },\n",
    "            'Random Forest': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 20, 30, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            },\n",
    "            'Gradient Boosting': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.05, 0.1, 0.2],\n",
    "                'max_depth': [3, 4, 5],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [3, 4, 5],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 1.0],\n",
    "                'colsample_bytree': [0.8, 1.0]\n",
    "            },\n",
    "            'Voting Classifier': {\n",
    "                'voting': ['hard', 'soft']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    elif task_type == 'regression':\n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'SVR': SVR(),\n",
    "            'Decision Tree Regressor': DecisionTreeRegressor(),\n",
    "            'Random Forest Regressor': RandomForestRegressor(),\n",
    "            'Gradient Boosting Regressor': GradientBoostingRegressor(),\n",
    "            'XGBoost Regressor': XGBRegressor(),\n",
    "            'Voting Regressor': VotingRegressor(estimators=[\n",
    "                ('lr', LinearRegression()),\n",
    "                ('rf', RandomForestRegressor()),\n",
    "                ('svr', SVR())\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        param_grids = {\n",
    "            'Linear Regression': {},\n",
    "            'SVR': {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'gamma': ['scale', 'auto']\n",
    "            },\n",
    "            'Decision Tree Regressor': {\n",
    "                'max_depth': [10, 20, 30, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            },\n",
    "            'Random Forest Regressor': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 20, 30, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            },\n",
    "            'Gradient Boosting Regressor': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.05, 0.1, 0.2],\n",
    "                'max_depth': [3, 4, 5],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            },\n",
    "            'XGBoost Regressor': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [3, 4, 5],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 1.0],\n",
    "                'colsample_bytree': [0.8, 1.0]\n",
    "            },\n",
    "            'Voting Regressor': {\n",
    "                'voting': ['hard', 'soft']\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Use TimeSeriesSplit for time series data\n",
    "    if time_series:\n",
    "        tscv = TimeSeriesSplit(n_splits=5)  # Can adjust the number of splits\n",
    "\n",
    "    # Train each model with hyperparameter tuning\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        \n",
    "        # Get the appropriate param grid for each model\n",
    "        param_grid = param_grids.get(model_name, {})\n",
    "        \n",
    "        # Use GridSearchCV or RandomizedSearchCV based on the search_type parameter\n",
    "        if search_type == 'grid':\n",
    "            search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv if time_series else 5, n_jobs=-1, verbose=2)\n",
    "        elif search_type == 'random':\n",
    "            search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, cv=tscv if time_series else 5, n_jobs=-1, verbose=2, random_state=42)\n",
    "        \n",
    "        # Fit the model\n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get the best model\n",
    "        best_model = search.best_estimator_\n",
    "        \n",
    "        # Predict on the test set\n",
    "        if task_type == 'classification':\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            score = accuracy_score(y_test, y_pred)\n",
    "        else:\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            score = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Best Hyperparameters: {search.best_params_}\")\n",
    "        print(f\"Best Score: {score:.4f}\")\n",
    "        \n",
    "        best_models[model_name] = best_model\n",
    "        best_params[model_name] = search.best_params_\n",
    "        best_scores[model_name] = score\n",
    "\n",
    "    return best_models, best_params, best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d749061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_evaluation\n",
    "\n",
    "# Function to evaluate models for classification tasks\n",
    "def evaluate_classification_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate models for regression tasks\n",
    "def evaluate_regression_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"R^2 Score: {r2:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Regression: Actual vs Predicted\")\n",
    "    plt.show()\n",
    "\n",
    "# --- NEW: Time Series Evaluation ---\n",
    "def evaluate_time_series_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Plots the forecast against the actual values in a timeline format.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"R^2 Score: {r2:.4f}\")\n",
    "\n",
    "    # Plotting Timeline\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(len(y_test)), y_test.values, label='Actual', color='blue')\n",
    "    plt.plot(range(len(y_pred)), y_pred, label='Predicted', color='orange', linestyle='--')\n",
    "    plt.title('Time Series Forecasting: Actual vs Predicted')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f8fbc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_final_report\n",
    "\n",
    "# Function to summarize and select the best model\n",
    "def summarize_best_models(best_models, best_params, best_scores, task_type='classification'):\n",
    "    # Prepare the data for a summary DataFrame\n",
    "    summary = {\n",
    "        'Model': [],\n",
    "        'Best Hyperparameters': [],\n",
    "        'Performance Score': []\n",
    "    }\n",
    "    \n",
    "    # Iterate through the models to summarize performance\n",
    "    for model_name in best_models:\n",
    "        summary['Model'].append(model_name)\n",
    "        summary['Best Hyperparameters'].append(best_params[model_name])\n",
    "        if task_type == 'classification':\n",
    "            summary['Performance Score'].append(best_scores[model_name])  # Accuracy for classification\n",
    "        else:\n",
    "            summary['Performance Score'].append(best_scores[model_name])  # MSE or R^2 for regression\n",
    "    \n",
    "    # Convert summary into a DataFrame\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    \n",
    "    # Sort models based on performance (highest score for classification, lowest MSE for regression)\n",
    "    if task_type == 'classification':\n",
    "        summary_df = summary_df.sort_values(by='Performance Score', ascending=False)\n",
    "    else:\n",
    "        summary_df = summary_df.sort_values(by='Performance Score', ascending=True)\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70c8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully!\n",
      "Generating data profiling report and visualizations...\n",
      "Target column manually set to: 'TenYearCHD'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595ffedf259d4ce3aa09adde73c35088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 1230.63it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c379cec19744b04b782f70db34c708d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db59bad2b0a41b89c4d805abfc06107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8cd23cff5a438197a774959bae1038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data profiling report generated as 'data_profiling_report.html'\n",
      "Preprocessing data... (Target: TenYearCHD)\n",
      "Data preprocessing completed!\n",
      "Encoding target labels...\n",
      "Target labels encoded. Classes: [0 1]\n",
      "Training models (Time Series Mode: False)...\n",
      "\n",
      "Training Logistic Regression...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Hyperparameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 1}\n",
      "Best Score: 0.8561\n",
      "\n",
      "Training SVM...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Hyperparameters: {'kernel': 'rbf', 'gamma': 'auto', 'C': 1}\n",
      "Best Score: 0.8550\n",
      "\n",
      "Training Decision Tree...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best Hyperparameters: {'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 10}\n",
      "Best Score: 0.7972\n",
      "\n",
      "Training Random Forest...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Hyperparameters: {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': None}\n",
      "Best Score: 0.8550\n",
      "\n",
      "Training Gradient Boosting...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Hyperparameters: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 4, 'learning_rate': 0.05}\n",
      "Best Score: 0.8514\n",
      "\n",
      "Training XGBoost...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best Hyperparameters: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "Best Score: 0.8573\n",
      "\n",
      "Training Voting Classifier...\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best Hyperparameters: {'voting': 'hard'}\n",
      "Best Score: 0.8597\n",
      "Model training completed!\n",
      "Evaluating models...\n",
      "\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy: 0.8561\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92       725\n",
      "           1       0.53      0.07      0.12       123\n",
      "\n",
      "    accuracy                           0.86       848\n",
      "   macro avg       0.70      0.53      0.52       848\n",
      "weighted avg       0.81      0.86      0.80       848\n",
      "\n",
      "\n",
      "Evaluating SVM...\n",
      "Accuracy: 0.8550\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       725\n",
      "           1       0.50      0.01      0.02       123\n",
      "\n",
      "    accuracy                           0.85       848\n",
      "   macro avg       0.68      0.50      0.47       848\n",
      "weighted avg       0.80      0.85      0.79       848\n",
      "\n",
      "\n",
      "Evaluating Decision Tree...\n",
      "Accuracy: 0.7972\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.89       725\n",
      "           1       0.17      0.11      0.13       123\n",
      "\n",
      "    accuracy                           0.80       848\n",
      "   macro avg       0.52      0.51      0.51       848\n",
      "weighted avg       0.76      0.80      0.78       848\n",
      "\n",
      "\n",
      "Evaluating Random Forest...\n",
      "Accuracy: 0.8550\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92       725\n",
      "           1       0.50      0.06      0.10       123\n",
      "\n",
      "    accuracy                           0.85       848\n",
      "   macro avg       0.68      0.52      0.51       848\n",
      "weighted avg       0.81      0.85      0.80       848\n",
      "\n",
      "\n",
      "Evaluating Gradient Boosting...\n",
      "Accuracy: 0.8514\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92       725\n",
      "           1       0.41      0.06      0.10       123\n",
      "\n",
      "    accuracy                           0.85       848\n",
      "   macro avg       0.64      0.52      0.51       848\n",
      "weighted avg       0.80      0.85      0.80       848\n",
      "\n",
      "\n",
      "Evaluating XGBoost...\n",
      "Accuracy: 0.8573\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       725\n",
      "           1       1.00      0.02      0.03       123\n",
      "\n",
      "    accuracy                           0.86       848\n",
      "   macro avg       0.93      0.51      0.48       848\n",
      "weighted avg       0.88      0.86      0.79       848\n",
      "\n",
      "\n",
      "Evaluating Voting Classifier...\n",
      "Accuracy: 0.8597\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       725\n",
      "           1       0.83      0.04      0.08       123\n",
      "\n",
      "    accuracy                           0.86       848\n",
      "   macro avg       0.85      0.52      0.50       848\n",
      "weighted avg       0.86      0.86      0.80       848\n",
      "\n",
      "Generating final model report...\n",
      "\n",
      "Classification Models Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Hyperparameters</th>\n",
       "      <th>Performance Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>{'voting': 'hard'}</td>\n",
       "      <td>0.859670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 200, 'max_d...</td>\n",
       "      <td>0.857311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'solver': 'liblinear', 'penalty': 'l1', 'C': 1}</td>\n",
       "      <td>0.856132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>{'kernel': 'rbf', 'gamma': 'auto', 'C': 1}</td>\n",
       "      <td>0.854953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 10,...</td>\n",
       "      <td>0.854953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 5, ...</td>\n",
       "      <td>0.851415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>{'min_samples_split': 10, 'min_samples_leaf': ...</td>\n",
       "      <td>0.797170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model                               Best Hyperparameters  \\\n",
       "6    Voting Classifier                                 {'voting': 'hard'}   \n",
       "5              XGBoost  {'subsample': 0.8, 'n_estimators': 200, 'max_d...   \n",
       "0  Logistic Regression   {'solver': 'liblinear', 'penalty': 'l1', 'C': 1}   \n",
       "1                  SVM         {'kernel': 'rbf', 'gamma': 'auto', 'C': 1}   \n",
       "3        Random Forest  {'n_estimators': 100, 'min_samples_split': 10,...   \n",
       "4    Gradient Boosting  {'n_estimators': 100, 'min_samples_split': 5, ...   \n",
       "2        Decision Tree  {'min_samples_split': 10, 'min_samples_leaf': ...   \n",
       "\n",
       "   Performance Score  \n",
       "6           0.859670  \n",
       "5           0.857311  \n",
       "0           0.856132  \n",
       "1           0.854953  \n",
       "3           0.854953  \n",
       "4           0.851415  \n",
       "2           0.797170  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model report generated!\n"
     ]
    }
   ],
   "source": [
    "# ml_run\n",
    "\n",
    "def run_ml_pipeline(data_path, task_type='classification', search_type='grid', time_series=False, date_column=None, target_column=None):\n",
    "    # Step 1: Load Data\n",
    "    print(\"Loading data...\")\n",
    "    if 'csv' in data_path:\n",
    "        df = pd.read_csv(data_path)\n",
    "    elif 'xlsx' in data_path:\n",
    "        df = pd.read_excel(data_path)\n",
    "    print(\"Data loaded successfully!\")\n",
    "    \n",
    "    # Step 2: Data Profiling (Passing target_column now)\n",
    "    print(\"Generating data profiling report and visualizations...\")\n",
    "    # --- UPDATED LINE ---\n",
    "    target_column, categorical_columns, numeric_columns = data_visualization(df, target_column=target_column) \n",
    "    # --------------------\n",
    "\n",
    "    # Step 3: Preprocess Data\n",
    "    print(f\"Preprocessing data... (Target: {target_column})\")\n",
    "    X_train, X_test, y_train, y_test, preprocessor = preprocess_data(\n",
    "        df, target_column, categorical_columns, numeric_columns, \n",
    "        time_series=time_series, date_column=date_column\n",
    "    )\n",
    "    print(\"Data preprocessing completed!\")\n",
    "    \n",
    "    # Classification Encoding\n",
    "    if task_type == 'classification':\n",
    "        print(\"Encoding target labels...\")\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train = label_encoder.fit_transform(y_train)\n",
    "        y_test = label_encoder.transform(y_test)\n",
    "        print(f\"Target labels encoded. Classes: {label_encoder.classes_}\")\n",
    "\n",
    "    # Step 4: Train Models\n",
    "    print(f\"Training models (Time Series Mode: {time_series})...\")\n",
    "    best_models, best_params, best_scores = train_models(\n",
    "        X_train, X_test, y_train, y_test, \n",
    "        task_type=task_type, search_type=search_type, time_series=time_series\n",
    "    )\n",
    "    print(\"Model training completed!\")\n",
    "    \n",
    "    # Step 5: Evaluate Models\n",
    "    print(\"Evaluating models...\")\n",
    "    for model_name, model in best_models.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        if task_type == 'classification':\n",
    "            evaluate_classification_model(model, X_test, y_test)\n",
    "        elif task_type == 'regression':\n",
    "            if time_series:\n",
    "                evaluate_time_series_model(model, X_test, y_test)\n",
    "            else:\n",
    "                evaluate_regression_model(model, X_test, y_test)\n",
    "    \n",
    "    # Step 6: Generate Final Report\n",
    "    print(\"Generating final model report...\")\n",
    "    final_summary = summarize_best_models(best_models, best_params, best_scores, task_type=task_type)\n",
    "    tools.display_dataframe_to_user(name=f\"{task_type.capitalize()} Models Summary\", dataframe=final_summary)\n",
    "    print(\"Final model report generated!\")\n",
    "\n",
    "# Example of using the pipeline for classification task\n",
    "data_path = r'D:\\Programming\\Machine Learning Works\\Projects\\ML Syntax Automation\\framingham.csv' \n",
    "\n",
    "# Run the pipeline\n",
    "run_ml_pipeline(data_path, task_type='classification', search_type='random', time_series=False, target_column='TenYearCHD')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
